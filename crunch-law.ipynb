{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to learn some basic data analysis.  Before we dive in, however, some terminology.\n",
    "<p>\n",
    "**Prediction vs Inference**  <Br>\n",
    "Social scientists use statistics primarily to carry out causal inference, that is, to try and figure out whether X causes Y.  Does democracy make states less likely to go to war?  If you flash a nasty word before someone's eyes for a microsecond, do they do worse on tests?  There are lots of big difficulties with causal inference (the short version: unless you have a controlled experiment, and then replicate it, you're always at least a little unsure about causation, and usually a lot unsure).  \n",
    "<p>\n",
    "By contrast, the task of prediction is somewhat easier.  Does X predict Y?  It's (excuse the philosophy) more of an epistemic than a metaphysical question: if I know X, can I come to reliable beliefs about Y?  For example, if I know the parties, the politics of the judge, and what the lower court did, can I predict the ruling of the Supreme Court?  (With a bit more information, <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2463244\">yes, surprisingly often</a>.)  If I know the text of the document, and I have a bunch of training data (more on this below), can I predict whether a lawyer will find it responsive to the discovery request?  If I have the text of a contract, can I predict whether a judge will conclude that it's enforceable or not?  Prediction's a bit easier, and it's what we'll focus on; however, when we move to the policy analysis section of our introductory period, we'll also talk a bit about inference (in terms of estimating the causal impact of policy interventions).  \n",
    "<p>\n",
    "**Model**<br>\n",
    "A model is an abstract way of representing the possible relationships between data.  For example, linear regression is a very simple model --- it simply represents the relationship between predictor variables and output variables as a line, i.e., as a function of the form $$y = ax_1 + bx_2 + cx_3...$$ (plus an \"intercept\", plus random error).  There are lots more complex models.  We say that we *fit* a model to the data, where the process of fitting means choosing the predictor variables (or \"features\") and choosing the general functional form in which they are to be represented.  Then the computer does the rest in algorithmic fashion---for example, when fitting a linear regression the computer uses optimization algorithms to find parameters (the a, b, c... above) that minimize the sum of squared distances (this is called linear regression's \"loss function\") between the predicted values (the values of y that result from plugging in the x-es to the function with a given value of a, b, c...) and the actual values.  \n",
    "<p>\n",
    "We evaluate models by their accuracy, that is, how well they predict training data (more on this in a moment), and, ultimately, how well they predict the unlabelled data you have to work with (assuming you ultimately learn the right answer for such things).  There are a variety of metrics for accuracy, and often times the particular problem you're trying to solve makes a difference (for example, if you're trying to predict life-threatening medical problems, false negatives are *lots worse* than false positives), but they all amount to different versions of \"we want fewer wrong answers and more right answers.\" \n",
    "<p>\n",
    "Different kinds of models are appropriate for the two big prediction tasks: *classification* (figuring out which bucket some datum goes into: is this document responsive to the discovery request or not?), and *regression* (figuring out what value on some continuous or continuous-ish scale a datum takes: how much money does this person make?  how old is she?)\n",
    "<p>\n",
    "**Different kinds of data**<br>\n",
    "*Labelled* data is data where you know both the predictors and the variable you're trying to predict.  (You know, for example, both the content of the document and whether or not the lawyer whose judgments you're trying to model has decided it's responsive.)  It's the stuff you start with.  *Unlabelled* data is data that lacks the latter (you just have the document, and you're trying to use your classification model [or \"classifier\"] to figure out whether it's responsive.  \n",
    "<p>\n",
    "Invariably, in fitting these models, you will find yourself dividing your store of labelled data into *training* data and *test* data.  Training data is the data you actually feed into your model in order to fit it, i.e., in order to choose the values for a, b, c, etc.  Test data is the data you use to validate your model.  Why do you need this?  Well... \n",
    "<p>\n",
    "**Overfitting, bias-variance tradeoff**<br>\n",
    "You need it because the big problem in prediction is known as \"overfitting.\"  In principle, there's always some model that will get arbitrarily small loss functions for any given pool of data.  At the limit, your \"function\" can just be a straight-up one-to-one mapping from the feature values in your data to the labels.  But if you fit a model like that (which is called \"overfitting\"), then it'll be totally useless when it comes to predicting unlabelled data, obviously.  One rough way to think of that is that the model adds no new information---you have to assume *some* kind of functional form or you're just repeating what you already know.  Another way to think of it is that when you overfit like this, you're basing your predictions on random noise in the original data.  (Still another way to think of it, and one that will probably cause the souls of a thousand lousy data scientists to rise up in rage and seek vengeance for my uttering it---but you know what, homie? talk to some actual scientists---is that there's still an implicit causal idea running underneath even predictive modeling, in the form of an actual attempt to capture whatever force it is that reliably makes your features and your labels go together.  But *anyway*.)\n",
    "<p>\n",
    "This leads right into the problem known as the *bias-variance tradeoff*.  Here, \"variance\" means the same thing as \"risk of overfitting,\" that is, the drop-off in accuracy between the labelled data you have and the unlabelled data you want to make sense of.  \"Bias,\" by contrast, is roughly speaking how much accuracy you can get on your labelled data. Roughly speaking, if you fit a model with a simple functional form, you're not all that likely to overfit, because you've thrown out a lot of information about the features.  Take linear regression again: lots of actual relationships don't come in linear form.  The real relationship might be a curve, like $$y=ax_1 + bx^2_2...$$ etc.  The predictive power you lose from shoving a quadratic relationship into linear form is bias.  (As I said, this is all pretty rough.  <a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\">Here's</a> a bit deeper of a discussion.)\n",
    "<p>\n",
    "Both bias and variance are bad.  But, in general, more complex models will have higher variance and lower bias, and vice versa for simpler models.  That's why it's a tradeoff: you pay with one problem for reducing the other.  There's no magic solution to the bias-variance tradeoff, but there are practical things one can do to lower variance, in particular (thus allowing one to use more complex models that reduce bias).  This is where training and testing data come in: the standard simple practice is to \"hold out\" some portion of your labelled data (usually between a quarter and a third) as a \"testing set,\" then fit your models based on the rest (the \"training set\").  Then, you can judge the quality of the various models you try out (the whole process is trying out a bunch of models and seeing what works best) by using their accuracy on the *testing* set.  Since, if you're overfitting, you're overfitting based on the training set (ahem, maybe... there might also be consistent noise through all your data... once again realscience inferencey problems rear their ugly heads in datascience predictionland.  but *anyway*), holding out a test set allows one to reduce the risk of overfitting.  There are also fancier techniques, like \"cross-validation,\" which basically means iteratively holding out a bunch of test sets then sticking them back in again.\n",
    "<p>\n",
    "As a general principle, adding more predictor variables (a.k.a. \"features,\" \"a.k.a.\" dimensions) increases the complexity of your model; one important aspect of the bias-variance tradeoff is deciding whether to throw out the information from extra features or include them; there are techniques to choose (e.g. dimensionality reduction techniques like principal components analysis, information-theoretic techniques like the <a href=\"https://en.wikipedia.org/wiki/Akaike_information_criterion\">akaike information criterion</a>---but don't worry about these now).  Also, as a general principle, more *observations* (not variables) in your training set is always better.  But the price of more observations is higher computational demands; the whole \"big data\" revolution is basically about figuring out how to process datasets with millions or billions of rows.  (The answer is \"use lots of computers at once, and fancy techniques to break up the data while still doing stats on it.  People get paid lots of money for that.)\n",
    "<p>\n",
    "Ok, enough talk.  If you want to get deeper into this (and to do so in a different programming language, R, which is also great, but I prefer Python because it's more useful for non-data things---and also because the main text-mining package for R, which is important for legal data, frankly sucks), go read <a href=\"http://www-bcf.usc.edu/~gareth/ISL/\">Introduction to Statistical Learning.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, grab the following dataset [[LINK]], which is a version of a selection of enron e-mails made available <a href=\"http://bailando.sims.berkeley.edu/enron_email.html\">here</a> and many other places.  Wikipedia has <a href=\"https://en.wikipedia.org/wiki/Enron_Corpus\">the story behind this dataset</a>.   I've carried out some cleanup tasks on this dataset---basically, I've gone through and converted the labels to a simple binary, which captures whether the email is about regulatory matters or not.  \n",
    "\n",
    "\n",
    "[structure: \n",
    "- simple example to capture basic process and ideas, logistic regression on enron data.  don't bother with underlying math.  based on bag of words.  just walk through this example to show what can be done.  \n",
    "- then toss off a couple other models.  perhaps knn, trees, and random forest.  \n",
    "- then close with \"did this inspire you?  go deeper.\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells have the code I used to directly download and clean the dataset.  **Please don't run them** (I'll tell you when to start again); there's no reason to create unnecessary load on someone else's server, but I wanted you to see it.  (Also, they do filesystem operations that could, in the unlikely event you have stuff with the same names, overwrite your personal data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib, os, tarfile, json\n",
    "\n",
    "def msgToTupe(folder, message, fnum):\n",
    "    msgfile = folder + message + '.txt'\n",
    "    catsfile = folder + message + '.cats'\n",
    "    with open(msgfile) as messagefile:\n",
    "        msg = messagefile.read()\n",
    "    with open(catsfile) as categoriesfile:\n",
    "        catlines = categoriesfile.readlines()\n",
    "    labels = [line[0] + line[2] for line in catlines]\n",
    "    if '31' in labels:\n",
    "        shortlabel = 1\n",
    "    else:\n",
    "        shortlabel = 0\n",
    "\n",
    "    return (shortlabel, labels, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('enron_small.tar.gz', <httplib.HTTPMessage instance at 0x10aad78c0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.mkdir(\"enron-policylab\")\n",
    "enronfile = urllib.URLopener()\n",
    "enronfile.retrieve(\"http://bailando.sims.berkeley.edu/enron/enron_with_categories.tar.gz\", \"enron_small.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf=tarfile.open(\"enron_small.tar.gz\", 'r')\n",
    "tf.extractall(\"enron-policylab\")\n",
    "tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails = []\n",
    "for folder in range(1, 9):\n",
    "    fname = \"enron-policylab/enron_with_categories/\" + str(folder) + '/'\n",
    "    ffiles = os.listdir(fname)\n",
    "    dupPrefixes = [x.split(\".\")[0] for x in ffiles]\n",
    "    prefixes = list(set(dupPrefixes))\n",
    "    for messageid in prefixes:\n",
    "        emails.append(msgToTupe(fname, messageid, folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.183901292597\n"
     ]
    }
   ],
   "source": [
    "# check to make sure there's enough variation in the label I chose:\n",
    "justlabels = [x[0] for x in emails]\n",
    "from numpy import mean as tempmean\n",
    "print tempmean(justlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# looks good to me.  Ok, let's strip out the unnecessary labels and get it into a usable JSON.\n",
    "strippedData = [(x[0], x[2]) for x in emails]\n",
    "with open('enronjson.json', 'w') as outfile:\n",
    "    json.dump(strippedData, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we're back.  The next few cells, you should go ahead and run.  It'll read the CSV into memory so you have data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('enronjson.json', 'r') as infile:\n",
    "    enronEmails = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, u'Message-ID: <20625717.1075857797770.JavaMail.evans@thyme>\\r\\nDate: Wed, 13 Dec 2000 06:03:00 -0800 (PST)\\r\\nFrom: thane.twiggs@enron.com\\r\\nTo: jeffery.ader@enron.com, mark.bernstein@enron.com, scott.healy@enron.com, \\r\\n\\tjanelle.scheuer@enron.com, tom.dutta@enron.com, dana.davis@enron.com, \\r\\n\\tpaul.broderick@enron.com, chris.dorland@enron.com, \\r\\n\\tgautam.gupta@enron.com, michael.brown@enron.com, \\r\\n\\tjohn.llodra@enron.com, george.wood@enron.com, joe.gordon@enron.com, \\r\\n\\tstephen.plauche@enron.com, jennifer.stewart@enron.com, \\r\\n\\tdavid.guillaume@enron.com, tom.may@enron.com, \\r\\n\\trobert.stalford@enron.com, jeffrey.miller@enron.com, \\r\\n\\tnarsimha.misra@enron.com, joe.quenet@enron.com, \\r\\n\\tpaul.thomas@enron.com, ricardo.perez@enron.com, \\r\\n\\tkevin.presto@enron.com, sarah.novosel@enron.com, \\r\\n\\tchristi.nicolay@enron.com\\r\\nSubject: ISO-NE failure to mitigate ICAP market -- Release of ISO NE\\r\\n confidential information\\r\\nMime-Version: 1.0\\r\\nContent-Type: text/plain; charset=us-ascii\\r\\nContent-Transfer-Encoding: 7bit\\r\\nX-From: Thane Twiggs\\r\\nX-To: Jeffery Ader, Mark Bernstein, Scott Healy, Janelle Scheuer, Tom Dutta, Dana Davis, Paul J Broderick, Chris Dorland, Gautam Gupta, Michael Brown, John Llodra, George Wood, Joe Gordon, Stephen Plauche, Jennifer N Stewart, David Guillaume, Tom May, Robert Stalford, Jeffrey Miller, Narsimha Misra, Joe Quenet, Paul D Thomas, Ricardo Perez, Kevin M Presto, Sarah Novosel, Christi L Nicolay\\r\\nX-cc: \\r\\nX-bcc: \\r\\nX-Folder: \\\\Joseph_Quenet_Dec2000\\\\Notes Folders\\\\All documents\\r\\nX-Origin: Quenet-J\\r\\nX-FileName: jquenet.nsf\\r\\n\\r\\nThe New England Conference of Public Utilities Commissioners (NECUPUC) filed \\nan Answer if Support of the Motion of the Maine Public Utilities Commission \\nfor Disclosure of Information.\\nNECUPUC supports the request for the release of the unredacted copies of \\nISO-NE\\'s September 21, 2000 Answer in this case.  In the alternative, they \\nwould ask that the Commission provide to the regulators that are parties to \\nthe proceeding unredacted copies of the ISO\\'s September 21, 2000 Answer \\nsubject to an appropriate protective order.\\n\\n\\n\\nDuke Energy North America (DENA) filed an Answer that opposes the MPUC \\nrequest for public information.\\n\\nDENA argues that only a  three month lag in the release of confidential \\ninformation is impermissible under a prior FERC ruling in the NSTAR Services \\nCo. case which set out a six-month lag rule for the release of information.\\nTheir second argument was that the request seeks information for all NEPOOL \\nmarkets and not just the ICAP market which is subject to the suit.\\nIf the FERC authorizes the release of confidential information, then it \\nshould be subject to a protective order which contains the following:\\nThe information may only be used for the purposes of this docket.\\nOnly specifically named \"reviewing Reps associated with the MPUC may review \\nthe information.\\nThe confidential materials may not be removed from the NE-ISO\\'s premises.\\nThe reviewing rep must execute a nondisclosure certificate.\\n\\n\\nAnswer to the MPUC\\'s Motion for Disclosure of Information from Northeast \\nUtilities Service Company and Select Energy and request for expedited \\ncommission action.  NUSCO and Select Energy Support MPUC\\'s request for \\ndisclosure of information that the ISO has filed under seal, but opposes the \\nselective disclosure of this information to the MPUC and other regulatory \\ncommissions and not other participants.   NUSCO and Select Energy request \\nexpedited action due the financial implications and there is also \\nconsiderable uncertainty regarding prices in the residual ICAP market in \\nJanuary, February and March of 2000 due to the suspended settlement pending \\nCommission guidance.\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at one.  you'll see it's in list format, where the first item is the label, and the second \n",
    "# is the text of the message w/ headers\n",
    "print enronEmails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1702\n"
     ]
    }
   ],
   "source": [
    "# how many messages do we have?\n",
    "print len(enronEmails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's tee up some more libraries that may be useful to us.  As I write this, I'm not sure whether \n",
    "# we'll need them all, especially pandas, but it's the standard python data stack.  \n",
    "# the import X as Y syntax, incidentlly, just renames the library in your local namespace\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import email as emem\n",
    "import nltk, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic way we turn text into data is a technique known as \"bag of words.\"  As the name suggests, we just treat a given piece of text as an undifferentiated glop of words, and then we see if we can predict the variable of interest (here, whether a human categorized it as about regulation or not) from whether or not words appear.  There are also a variety of slightly more sophisticated tweaks to bag of words, like looking at whether words come together (\"bigrams,\" = groups of 2, \"trigrams,\" etc.; generally, \"n-grams\"), various kinds of weighting techniques (most popularly <a href=\"http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/\">tf-idf scores</a>), etc.  But let's keep it simple here and just work with the most basic version.\n",
    "\n",
    "So let's talk about that.  We have a pile of text.  We want to accomplish preparatory tasks: \n",
    "1.  Get it into a standard tabular data representation (where each colum is a word is a variable, and each row is a document, and then there's a 1 in the corresponding cell if the document contains the word, and an 0 otherwise.\n",
    "2.  Get rid of the garbage: we don't want to include words like \"is\" and \"and\" because they're unlikely to be meaningful (and the more words we have, the more complex our model is; see above re: overfitting---too many dimensions = bad) (\"dimensions\" = \"features\").  These are called \"stopwords.\"  For similar reasons, we probably want to \"stem\" words---to get rid of suffixes and such that differentiate words without changing the meaning (\"quicker\" vs \"quickly\" etc.), get rid of punctuation, make everything lowercase, etc.  To be clear, we're still throwing out information here (maybe there's a difference between texts where someone writes in all caps, and texts where they're not), but we're throwing out low-payoff information.  If we had hundreds of thousands of emails, maybe we'd keep them in.  Because we're dealing with emails, and we won't be making use of information about headers and such, it will also be useful to get rid of the labels for those headers and such.\n",
    "\n",
    "There are libraries for all this stuff, but it's actually easier to just implement most of it straight; as we've already seen, Python has power text processing capabilities.  So the next cell is a simple function that just does all that, although we will use an e-mail processing library...\n",
    "\n",
    "Note that stemming in particular <a href=\"http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\">is really dirty</a>---because English isn't all that regular a language, you get lots of stupid results, non-equivalent words equated, in any of the numerous algorithms available.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First, let's download a list of stopwords, and then let's look at one example of a list of words with all the garbage removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'jefferi', u'ader', u'mark', u'bernstein', u'scott', u'heali', u'janel', u'scheuer', u'tom', u'dutta', u'dana', u'davi', u'paul', u'broderick', u'chri', u'dorland', u'gautam', u'gupta', u'michael', u'brown', u'john', u'llodra', u'georg', u'wood', u'joe', u'gordon', u'stephen', u'plauch', u'jennif', u'stewart', u'david', u'guillaum', u'tom', u'may', u'robert', u'stalford', u'jeffrey', u'miller', u'narsimha', u'misra', u'joe', u'quenet', u'paul', u'thoma', u'ricardo', u'perez', u'kevin', u'presto', u'sarah', u'novosel', u'christi', u'nicolay', u'thane', u'twigg', u'iso', u'ne', u'failur', u'mitig', u'icap', u'market', u'releas', u'iso', u'ne', u'confidenti', u'inform', u'new', u'england', u'confer', u'public', u'util', u'commission', u'necupuc', u'file', u'answer', u'support', u'motion', u'main', u'public', u'util', u'commiss', u'disclosur', u'inform', u'necupuc', u'support', u'request', u'releas', u'unredact', u'copi', u'iso', u'ne', u'septemb', u'answer', u'case', u'altern', u'would', u'ask', u'commiss', u'provid', u'regul', u'parti', u'proceed', u'unredact', u'copi', u'iso', u'septemb', u'answer', u'subject', u'appropri', u'protect', u'order', u'duke', u'energi', u'north', u'america', u'dena', u'file', u'answer', u'oppos', u'mpuc', u'request', u'public', u'inform', u'dena', u'argu', u'three', u'month', u'lag', u'releas', u'confidenti', u'inform', u'impermiss', u'prior', u'ferc', u'rule', u'nstar', u'servic', u'co', u'case', u'set', u'six', u'month', u'lag', u'rule', u'releas', u'inform', u'second', u'argument', u'request', u'seek', u'inform', u'nepool', u'market', u'icap', u'market', u'subject', u'suit', u'ferc', u'author', u'releas', u'confidenti', u'inform', u'subject', u'protect', u'order', u'contain', u'follow', u'inform', u'may', u'use', u'purpos', u'docket', u'specif', u'name', u'review', u'rep', u'associ', u'mpuc', u'may', u'review', u'inform', u'confidenti', u'materi', u'may', u'remov', u'ne', u'iso', u'premis', u'review', u'rep', u'must', u'execut', u'nondisclosur', u'certif', u'answer', u'mpuc', u'motion', u'disclosur', u'inform', u'northeast', u'util', u'servic', u'compani', u'select', u'energi', u'request', u'expedit', u'commiss', u'action', u'nusco', u'select', u'energi', u'support', u'mpuc', u'request', u'disclosur', u'inform', u'iso', u'file', u'seal', u'oppos', u'select', u'disclosur', u'inform', u'mpuc', u'regulatori', u'commiss', u'particip', u'nusco', u'select', u'energi', u'request', u'expedit', u'action', u'due', u'financi', u'implic', u'also', u'consider', u'uncertainti', u'regard', u'price', u'residu', u'icap', u'market', u'januari', u'februari', u'march', u'due', u'suspend', u'settlement', u'pend', u'commiss', u'guidanc']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "# first a function to extract all the content and dump header labels, i.e., to \"parse\" the e-mail\n",
    "def parseEmail(document):\n",
    "    theMessage = emem.message_from_string(document)\n",
    "    tofield = theMessage['to']\n",
    "    fromfield = theMessage['from']\n",
    "    subjectfield = theMessage['subject']\n",
    "    bodyfield = theMessage.get_payload()\n",
    "    wholeMsgList = [tofield, fromfield, subjectfield, bodyfield]\n",
    "    # get rid of any fields that don't exist in the email\n",
    "    cleanMsgList = [x for x in wholeMsgList if x is not None]\n",
    "    # now return a string with all that stuff run together\n",
    "    return ' '.join(cleanMsgList)\n",
    "\n",
    "# get rid of anything that isn't a letter -- see here for explanation of how regular expresisons do this: \n",
    "# https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "def lettersOnly(document):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", document)\n",
    "    \n",
    "def wordBag(document):\n",
    "    return lettersOnly(parseEmail(document)).lower().split()\n",
    "\n",
    "def cleanDoc(document):\n",
    "    dasbag = wordBag(document)\n",
    "    # get rid of \"enron\" for obvious reasons, also the .com\n",
    "    bagB = [word for word in dasbag if not word in ['enron','com']]\n",
    "    unstemmed =[word for word in bagB if not word in stopwords.words(\"english\")]\n",
    "    return [stemmer.stem(word) for word in unstemmed]\n",
    "    \n",
    "    \n",
    "print cleanDoc(enronEmails[0][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's cut things down a little further.  I'd also like to get rid of words less than three letters.  After doing that, I'm going to be obnoxious and indirect and turn our nice list of words back into a string again.  Why?  Because the main python data-crunching package has a nice tool to turn strings into a \"document term matrix\" (our tabular representation) as well as get rid of words that almost never appear.  So we're going back and forth between strings and lists here, which is kind of silly, and lots of this stuff could have been done more concisely and quickly, but I'm trying to make all the steps explicit and clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jefferi ader mark bernstein scott heali janel scheuer tom dutta dana davi paul broderick chri dorland gautam gupta michael brown john llodra georg wood joe gordon stephen plauch jennif stewart david guillaum tom may robert stalford jeffrey miller narsimha misra joe quenet paul thoma ricardo perez kevin presto sarah novosel christi nicolay thane twigg iso failur mitig icap market releas iso confidenti inform new england confer public util commission necupuc file answer support motion main public util commiss disclosur inform necupuc support request releas unredact copi iso septemb answer case altern would ask commiss provid regul parti proceed unredact copi iso septemb answer subject appropri protect order duke energi north america dena file answer oppos mpuc request public inform dena argu three month lag releas confidenti inform impermiss prior ferc rule nstar servic case set six month lag rule releas inform second argument request seek inform nepool market icap market subject suit ferc author releas confidenti inform subject protect order contain follow inform may use purpos docket specif name review rep associ mpuc may review inform confidenti materi may remov iso premis review rep must execut nondisclosur certif answer mpuc motion disclosur inform northeast util servic compani select energi request expedit commiss action nusco select energi support mpuc request disclosur inform iso file seal oppos select disclosur inform mpuc regulatori commiss particip nusco select energi request expedit action due financi implic also consider uncertainti regard price residu icap market januari februari march due suspend settlement pend commiss guidanc\n"
     ]
    }
   ],
   "source": [
    "def atLeastThreeString(cleandoc):\n",
    "    return ' '.join([w for w in cleandoc if len(w)>2])\n",
    "print atLeastThreeString(cleanDoc(enronEmails[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "justEmails = [email[1] for email in enronEmails]\n",
    "bigEmailsList = [atLeastThreeString(cleanDoc(email)) for email in justEmails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jefferi ader mark bernstein scott heali janel scheuer tom dutta dana davi paul broderick chri dorland gautam gupta michael brown john llodra georg wood joe gordon stephen plauch jennif stewart david guillaum tom may robert stalford jeffrey miller narsimha misra joe quenet paul thoma ricardo perez kevin presto sarah novosel christi nicolay thane twigg iso failur mitig icap market releas iso confidenti inform new england confer public util commission necupuc file answer support motion main public util commiss disclosur inform necupuc support request releas unredact copi iso septemb answer case altern would ask commiss provid regul parti proceed unredact copi iso septemb answer subject appropri protect order duke energi north america dena file answer oppos mpuc request public inform dena argu three month lag releas confidenti inform impermiss prior ferc rule nstar servic case set six month lag rule releas inform second argument request seek inform nepool market icap market subject suit ferc author releas confidenti inform subject protect order contain follow inform may use purpos docket specif name review rep associ mpuc may review inform confidenti materi may remov iso premis review rep must execut nondisclosur certif answer mpuc motion disclosur inform northeast util servic compani select energi request expedit commiss action nusco select energi support mpuc request disclosur inform iso file seal oppos select disclosur inform mpuc regulatori commiss particip nusco select energi request expedit action due financi implic also consider uncertainti regard price residu icap market januari februari march due suspend settlement pend commiss guidanc\n"
     ]
    }
   ],
   "source": [
    "print bigEmailsList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ignore this cell; I had to load and unload the data a few times.  \n",
    "import json\n",
    "with open('cleanfullenronds.json', 'r') as infile:\n",
    "    enronEmailsClean = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, u'jefferi ader mark bernstein scott heali janel scheuer tom dutta dana davi paul broderick chri dorland gautam gupta michael brown john llodra georg wood joe gordon stephen plauch jennif stewart david guillaum tom may robert stalford jeffrey miller narsimha misra joe quenet paul thoma ricardo perez kevin presto sarah novosel christi nicolay thane twigg iso failur mitig icap market releas iso confidenti inform new england confer public util commission necupuc file answer support motion main public util commiss disclosur inform necupuc support request releas unredact copi iso septemb answer case altern would ask commiss provid regul parti proceed unredact copi iso septemb answer subject appropri protect order duke energi north america dena file answer oppos mpuc request public inform dena argu three month lag releas confidenti inform impermiss prior ferc rule nstar servic case set six month lag rule releas inform second argument request seek inform nepool market icap market subject suit ferc author releas confidenti inform subject protect order contain follow inform may use purpos docket specif name review rep associ mpuc may review inform confidenti materi may remov iso premis review rep must execut nondisclosur certif answer mpuc motion disclosur inform northeast util servic compani select energi request expedit commiss action nusco select energi support mpuc request disclosur inform iso file seal oppos select disclosur inform mpuc regulatori commiss particip nusco select energi request expedit action due financi implic also consider uncertainti regard price residu icap market januari februari march due suspend settlement pend commiss guidanc']\n"
     ]
    }
   ],
   "source": [
    "print enronEmailsClean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 7.0.1, however version 7.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting textmining\n",
      "  Downloading textmining-1.0.zip (1.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.9MB 251kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: textmining\n",
      "  Running setup.py bdist_wheel for textmining\n",
      "  Stored in directory: /Users/pauliglot/Library/Caches/pip/wheels/fa/8e/91/b26dd14a741d468affdfb97eb93928bedccf685c44a4a9f609\n",
      "Successfully built textmining\n",
      "Installing collected packages: textmining\n",
      "Successfully installed textmining-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textmining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textmining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdm = textmining.TermDocumentMatrix()\n",
    "for tempLinevariable in enronEmailsClean:\n",
    "    tdm.add_doc(tempLinevariable[1])\n",
    "tdm.write_csv('fullEnronDTM.csv', cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a list of rows from the document term matrix, cutting off words that appear in fewer than 85 \n",
    "# documents (5%), and appending the label to each row.  \n",
    "enronLabelsOnly = [x[0] for x in enronEmailsClean]\n",
    "enronLabelsOnly.insert(0, 'LABELS')\n",
    "enronWorkingData = []\n",
    "for index, value in enumerate(tdm.rows(cutoff=85)):\n",
    "    value.append(enronLabelsOnly[index])\n",
    "    enronWorkingData.append(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enronWorkingData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'four', u'wednesday', u'budget', u'second', u'deregul', u'brought', u'unit', u'spoke', u'relat', u'notic', u'hold', u'want', u'turn', u'hot', u'hou', u'wrong', u'pretti', u'fix', u'commiss', u'jennif', u'unabl', u'recent', u'legislatur', u'project', u'object', u'letter', u'came', u'bush', u'busi', u'rick', u'respond', u'fair', u'result', u'respons', u'fail', u'best', u'figur', u'extend', u'extens', u'debt', u'countri', u'assum', u'much', u'regul', u'wish', u'dave', u'davi', u'commerci', u'credit', u'legisl', u'measur', u'specif', u'attorney', u'right', u'old', u'transmiss', u'condit', u'support', u'avail', u'joseph', u'offer', u'exist', u'floor', u'role', u'roll', u'intend', u'intent', u'time', u'push', u'chair', u'decid', u'decis', u'team', u'prevent', u'sign', u'current', u'address', u'along', u'prefer', u'peopl', u'behalf', u'whatev', u'materi', u'spot', u'date', u'data', u'jim', u'nation', u'didn', u'separ', u'internet', u'million', u'oper', u'one', u'vote', u'open', u'citi', u'draft', u'san', u'say', u'note', u'take', u'noth', u'drive', u'wind', u'transact', u'activ', u'requir', u'roger', u'shortag', u'mani', u'west', u'former', u'pursu', u'econom', u'know', u'summer', u'rest', u'littl', u'exercis', u'around', u'world', u'manag', u'power', u'act', u'industri', u'mean', u'other', u'effici', u'conclud', u'pull', u'creat', u'certain', u'congress', u'mcvicker', u'consider', u'harri', u'small', u'past', u'pass', u'richard', u'section', u'full', u'prior', u'action', u'depart', u'morn', u'compani', u'kaminski', u'learn', u'challeng', u'accept', u'plant', u'trade', u'paper', u'scott', u'found', u'realli', u'research', u'believ', u'major', u'number', u'natur', u'consult', u'arrang', u'comput', u'packag', u'sell', u'also', u'play', u'plan', u'cover', u'session', u'impact', u'writer', u'solut', u'set', u'see', u'sen', u'last', u'whole', u'load', u'schedul', u'firm', u'fire', u'fund', u'technolog', u'implement', u'person', u'expens', u'agreement', u'submit', u'custom', u'suit', u'link', u'petrochko', u'line', u'mitig', u'ago', u'send', u'sens', u'sent', u'tri', u'imceanot', u'let', u'residenti', u'great', u'opinion', u'commun', u'next', u'process', u'high', u'delay', u'seriou', u'issu', u'allow', u'houston', u'move', u'overal', u'strateg', u'anyth', u'francisco', u'develop', u'sever', u'regulatori', u'linnel', u'david', u'outsid', u'owner', u'system', u'haven', u'clearli', u'linda', u'soar', u'face', u'fact', u'bring', u'economist', u'jan', u'hope', u'email', u'etc', u'mailto', u'immedi', u'togeth', u'site', u'greater', u'competit', u'upon', u'expand', u'exampl', u'less', u'paul', u'web', u'five', u'corpor', u'crisi', u'dynegi', u'feder', u'avoid', u'stage', u'sander', u'otherwis', u'problem', u'novemb', u'inc', u'monday', u'rule', u'worth', u'told', u'studi', u'total', u'sarah', u'word', u'work', u'could', u'indic', u'interview', u'lay', u'law', u'order', u'offici', u'offic', u'bank', u'standard', u'renew', u'independ', u'john', u'provid', u'pressur', u'minut', u'provis', u'opposit', u'identifi', u'involv', u'secretari', u'septemb', u'awar', u'away', u'accord', u'howev', u'speak', u'revenu', u'properti', u'air', u'evid', u'hand', u'rais', u'thu', u'client', u'jose', u'board', u'night', u'plu', u'confer', u'post', u'mahoney', u'opportun', u'way', u'jone', u'consum', u'certainli', u'brown', u'octob', u'administr', u'guy', u'cost', u'appear', u'gener', u'market', u'live', u'prepar', u'cap', u'focu', u'cal', u'heard', u'occur', u'economi', u'write', u'product', u'aol', u'southern', u'approv', u'still', u'non', u'half', u'hall', u'drop', u'januari', u'bradley', u'year', u'happen', u'argu', u'care', u'directli', u'tomorrow', u'especi', u'angel', u'deliv', u'begin', u'price', u'america', u'cannot', u'concern', u'import', u'blame', u'mccubbin', u'mona', u'senat', u'media', u'ceo', u'document', u'someon', u'earli', u'read', u'maureen', u'rebecca', u'benefit', u'central', u'area', u'lon', u'start', u'low', u'lot', u'hire', u'stanford', u'peak', u'reduc', u'month', u'articl', u'smith', u'blackout', u'pacif', u'excess', u'strong', u'ahead', u'amount', u'real', u'purchas', u'hartso', u'almost', u'taken', u'anoth', u'help', u'soon', u'held', u'fulli', u'democrat', u'beyond', u'event', u'robert', u'publish', u'puc', u'reason', u'base', u'put', u'basi', u'american', u'lawmak', u'toward', u'clear', u'latest', u'copyright', u'close', u'particip', u'won', u'jeff', u'coordin', u'look', u'invit', u'governor', u'readi', u'grant', u'user', u'march', u'game', u'resolv', u'donna', u'run', u'step', u'block', u'within', u'incent', u'institut', u'similar', u'doesn', u'repres', u'guarante', u'clean', u'william', u'megawatt', u'michael', u'click', u'becom', u'convers', u'chang', u'chanc', u'win', u'control', u'georg', u'agenda', u'handl', u'special', u'manipul', u'ect', u'ask', u'keep', u'attach', u'final', u'deliveri', u'need', u'happi', u'urg', u'affair', u'fuel', u'buyer', u'gray', u'contain', u'statu', u'ken', u'key', u'monitor', u'polici', u'tuesday', u'quit', u'cent', u'revis', u'parti', u'speaker', u'northwest', u'http', u'effect', u'well', u'skill', u'loss', u'lost', u'necessari', u'page', u'home', u'peter', u'estim', u'encourag', u'washington', u'north', u'limit', u'eric', u'diego', u'futur', u'stay', u'decemb', u'whose', u'guerrero', u'govern', u'affect', u'even', u'neg', u'spokesman', u'new', u'net', u'ever', u'never', u'met', u'cindi', u'jame', u'permit', u'counti', u'campaign', u'call', u'recommend', u'robertson', u'type', u'tell', u'schroeder', u'warn', u'room', u'give', u'answer', u'negoti', u'updat', u'attempt', u'third', u'maintain', u'privileg', u'better', u'promis', u'went', u'side', u'bond', u'situat', u'dasovich', u'iso', u'isn', u'agre', u'lower', u'matter', u'seen', u'seem', u'seek', u'kean', u'doc', u'despit', u'explain', u'stop', u'bad', u'subject', u'said', u'three', u'commission', u'interest', u'basic', u'servic', u'near', u'receiv', u'make', u'skean', u'sacramento', u'left', u'yet', u'languag', u'nicolay', u'save', u'applic', u'www', u'deal', u'intern', u'dear', u'promot', u'afternoon', u'commit', u'analysi', u'form', u'forc', u'analyst', u'sale', u'billion', u'ignor', u'daili', u'mile', u'depend', u'lynch', u'item', u'wait', u'extrem', u'bob', u'elect', u'visit', u'effort', u'claim', u'kenneth', u'predict', u'council', u'agenc', u'may', u'grow', u'talk', u'entiti', u'group', u'thank', u'mail', u'main', u'financi', u'initi', u'continu', u'ensur', u'earlier', u'california', u'org', u'thing', u'think', u'first', u'carri', u'long', u'oppos', u'averag', u'kevin', u'advis', u'track', u'paid', u'show', u'black', u'nearli', u'variou', u'get', u'calendar', u'wonder', u'enough', u'volatil', u'across', u'august', u'trader', u'improv', u'among', u'mara', u'mari', u'mark', u'univers', u'sound', u'margin', u'advantag', u'pat', u'pay', u'jeffrey', u'oil', u'assist', u'appropri', u'execut', u'money', u'grid', u'either', u'confirm', u'possibl', u'file', u'network', u'dollar', u'util', u'fall', u'abl', u'public', u'valu', u'declin', u'establish', u'two', u'desir', u'hous', u'remain', u'secur', u'associ', u'late', u'pend', u'good', u'hard', u'idea', u'connect', u'energi', u'exchang', u'done', u'construct', u'statement', u'part', u'build', u'distribut', u'previou', u'charl', u'charg', u'commerc', u'particularli', u'fine', u'find', u'steff', u'express', u'vinc', u'reserv', u'expert', u'someth', u'experi', u'altern', u'appreci', u'annual', u'simpli', u'point', u'suppli', u'throughout', u'understand', u'bill', u'everyon', u'yesterday', u'spent', u'spend', u'cut', u'big', u'bid', u'often', u'back', u'per', u'larg', u'impos', u'bankruptci', u'steven', u'includ', u'forward', u'privat', u'refer', u'edison', u'access', u'peggi', u'meet', u'sinc', u'later', u'advoc', u'account', u'employe', u'onlin', u'stock', u'chief', u'dan', u'mention', u'day', u'februari', u'rel', u'frank', u'releas', u'presid', u'retail', u'south', u'attent', u'facil', u'messag', u'structur', u'mid', u'unless', u'payment', u'request', u'texa', u'staff', u'increas', u'organ', u'shapiro', u'progress', u'janet', u'janel', u'comment', u'electr', u'partner', u'center', u'thought', u'add', u'fyi', u'like', u'success', u'hain', u'although', u'panel', u'actual', u'biggest', u'buy', u'probabl', u'detail', u'everi', u'risk', u'rise', u'school', u'direct', u'street', u'conduct', u'supplier', u'forum', u'describ', u'would', u'asset', u'spike', u'phone', u'must', u'join', u'end', u'elizabeth', u'enter', u'choic', u'goe', u'got', u'gov', u'free', u'rang', u'alreadi', u'top', u'tom', u'took', u'western', u'conserv', u'ray', u'contact', u'though', u'flow', u'report', u'approach', u'news', u'protect', u'karen', u'quickli', u'tabl', u'suggest', u'complet', u'everyth', u'els', u'gave', u'cooper', u'corp', u'produc', u'duke', u'ferc', u'head', u'heat', u'hear', u'counsel', u'kaufman', u'check', u'assembl', u'tim', u'consid', u'longer', u'rob', u'signific', u'environment', u'sourc', u'hour', u'level', u'quick', u'repli', u'water', u'wholesal', u'today', u'capit', u'share', u'judg', u'purpos', u'critic', u'alway', u'anyon', u'huge', u'practic', u'inform', u'combin', u'refund', u'term', u'name', u'individu', u'profit', u'refus', u'place', u'origin', u'given', u'white', u'copi', u'alan', u'shirley', u'serv', u'wide', u'balanc', u'posit', u'ann', u'sure', u'senior', u'brigg', u'state', u'wood', u'review', u'comn', u'come', u'region', u'contract', u'period', u'coupl', u'andrew', u'case', u'amend', u'cash', u'author', u'week', u'confidenti', u'director', u'without', u'relief', u'except', u'legal', u'sherriff', u'stand', u'gari', u'regard', u'mike', u'belden', u'invest', u'potenti', u'lack', u'abil', u'follow', u'thursday', u'program', u'fax', u'global', u'far', u'list', u'rate', u'design', u'sue', u'brief', u'version', u'proceed', u'known', u'enronxg', u'cours', u'court', u'goal', u'rather', u'resourc', u'reflect', u'short', u'susan', u'caus', u'began', u'style', u'might', u'wouldn', u'return', u'expect', u'hill', u'advanc', u'friday', u'differ', u'perhap', u'feel', u'least', u'stori', u'option', u'kind', u'chairman', u'reach', u'sandra', u'hit', u'financ', u'due', u'strategi', u'demand', u'higher', u'propos', u'reliant', u'collect', u'pipelin', u'surpris', u'seller', u'palmer', u'novosel', u'announc', u'question', u'perform', u'consist', u'vice', u'steve', u'edu', u'pleas', u'compar', u'wolak', u'appli', u'greg', u'use', u'sort', u'tax', u'sit', u'six', u'instead', u'chri', u'attend', u'light', u'republican', u'investor', u'polit', u'york', u'capac', u'shelk', u'juli', u'deni', u'denn', u'determin', u'locat', u'local', u'contribut', u'view', u'favor', u'focus', u'job', u'entir', u'joe', u'addit', u'april', u'committe', u'wall', u'respect', u'present', u'will', u'member', u'largest', u'difficult', u'obtain', u'discuss', u'press', u'growth', u'lead', u'leav', u'leader', u'investig', u'own', u'owe', u'made', u'whether', u'troubl', u'record', u'percent', u'book', u'june', u'reliabl', u'emerg', 'LABELS']\n"
     ]
    }
   ],
   "source": [
    "print enronWorkingData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 12, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print enronWorkingData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1703"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enronWorkingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, finally, we're ready to have a dataset that's actually useful to us.  Let's just stick this sucker into \n",
    "# numpy---we don't need fancy pandas dataframes and such, since everything is a numeric variable...\n",
    "enronAnalysisData = np.array(enronWorkingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's save this as a csv, then I'm going to just put a jump link in here in order that students can skip the cleanup\n",
    "# and go right to the analysis.  \n",
    "enronAnalysisData.tofile('enronAnalysis.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 12, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print enronAnalysisData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# actually, that file output is terrible.  Forget it, using pandas.\n",
    "enronData = pd.DataFrame(enronAnalysisData[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2  [0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...\n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, ...\n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...\n"
     ]
    }
   ],
   "source": [
    "print enronData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1702, 1)\n"
     ]
    }
   ],
   "source": [
    "print enronData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ok, this is ridiculous.  I'm starting fresh here with a whole new method of constructing the dataset.  \n",
    "headers = enronWorkingData[0]\n",
    "values = enronWorkingData[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'four', u'wednesday', u'budget', u'second', u'deregul']\n"
     ]
    }
   ],
   "source": [
    "print headers[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 12, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "1060 columns passed, passed data had 1061 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-74c8065efb43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menronDataAnalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/pauliglot/.pyenv/versions/2.7.10/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauliglot/.pyenv/versions/2.7.10/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   4904\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4905\u001b[0m         return _list_to_arrays(data, columns, coerce_float=coerce_float,\n\u001b[0;32m-> 4906\u001b[0;31m                                dtype=dtype)\n\u001b[0m\u001b[1;32m   4907\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4908\u001b[0m         return _list_of_dict_to_arrays(data, columns,\n",
      "\u001b[0;32m/Users/pauliglot/.pyenv/versions/2.7.10/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   4987\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4988\u001b[0m     return _convert_object_array(content, columns, dtype=dtype,\n\u001b[0;32m-> 4989\u001b[0;31m                                  coerce_float=coerce_float)\n\u001b[0m\u001b[1;32m   4990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauliglot/.pyenv/versions/2.7.10/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_convert_object_array\u001b[0;34m(content, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   5045\u001b[0m             \u001b[0;31m# caller's responsibility to check for this...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5046\u001b[0m             raise AssertionError('%d columns passed, passed data had %s '\n\u001b[0;32m-> 5047\u001b[0;31m                                  'columns' % (len(columns), len(content)))\n\u001b[0m\u001b[1;32m   5048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5049\u001b[0m     \u001b[0;31m# provide soft conversion of object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 1060 columns passed, passed data had 1061 columns"
     ]
    }
   ],
   "source": [
    "enronDataAnalysis = pd.DataFrame(values, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABELS\n"
     ]
    }
   ],
   "source": [
    "print(headers[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four\n"
     ]
    }
   ],
   "source": [
    "print headers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enronWorkingData[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1703"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enronWorkingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1702"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enronEmailsClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1703"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enronLabelsOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newEnronBasic = []\n",
    "for index, value in enumerate(tdm.rows(cutoff=85)):\n",
    "    value.append(enronLabelsOnly[index])\n",
    "    newEnronBasic.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newEnronBasic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newEnronBasic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "templist =[]\n",
    "for tempitem in tdm.rows(cutoff=85):\n",
    "    templist.append(tempitem)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1059"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(templist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1059"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(templist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1703"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(templist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'four', u'wednesday', u'budget', u'second', u'deregul', u'brought', u'unit', u'spoke', u'relat', u'notic', u'hold', u'want', u'turn', u'hot', u'hou', u'wrong', u'pretti', u'fix', u'commiss', u'jennif', u'unabl', u'recent', u'legislatur', u'project', u'object', u'letter', u'came', u'bush', u'busi', u'rick', u'respond', u'fair', u'result', u'respons', u'fail', u'best', u'figur', u'extend', u'extens', u'debt', u'countri', u'assum', u'much', u'regul', u'wish', u'dave', u'davi', u'commerci', u'credit', u'legisl', u'measur', u'specif', u'attorney', u'right', u'old', u'transmiss', u'condit', u'support', u'avail', u'joseph', u'offer', u'exist', u'floor', u'role', u'roll', u'intend', u'intent', u'time', u'push', u'chair', u'decid', u'decis', u'team', u'prevent', u'sign', u'current', u'address', u'along', u'prefer', u'peopl', u'behalf', u'whatev', u'materi', u'spot', u'date', u'data', u'jim', u'nation', u'didn', u'separ', u'internet', u'million', u'oper', u'one', u'vote', u'open', u'citi', u'draft', u'san', u'say', u'note', u'take', u'noth', u'drive', u'wind', u'transact', u'activ', u'requir', u'roger', u'shortag', u'mani', u'west', u'former', u'pursu', u'econom', u'know', u'summer', u'rest', u'littl', u'exercis', u'around', u'world', u'manag', u'power', u'act', u'industri', u'mean', u'other', u'effici', u'conclud', u'pull', u'creat', u'certain', u'congress', u'mcvicker', u'consider', u'harri', u'small', u'past', u'pass', u'richard', u'section', u'full', u'prior', u'action', u'depart', u'morn', u'compani', u'kaminski', u'learn', u'challeng', u'accept', u'plant', u'trade', u'paper', u'scott', u'found', u'realli', u'research', u'believ', u'major', u'number', u'natur', u'consult', u'arrang', u'comput', u'packag', u'sell', u'also', u'play', u'plan', u'cover', u'session', u'impact', u'writer', u'solut', u'set', u'see', u'sen', u'last', u'whole', u'load', u'schedul', u'firm', u'fire', u'fund', u'technolog', u'implement', u'person', u'expens', u'agreement', u'submit', u'custom', u'suit', u'link', u'petrochko', u'line', u'mitig', u'ago', u'send', u'sens', u'sent', u'tri', u'imceanot', u'let', u'residenti', u'great', u'opinion', u'commun', u'next', u'process', u'high', u'delay', u'seriou', u'issu', u'allow', u'houston', u'move', u'overal', u'strateg', u'anyth', u'francisco', u'develop', u'sever', u'regulatori', u'linnel', u'david', u'outsid', u'owner', u'system', u'haven', u'clearli', u'linda', u'soar', u'face', u'fact', u'bring', u'economist', u'jan', u'hope', u'email', u'etc', u'mailto', u'immedi', u'togeth', u'site', u'greater', u'competit', u'upon', u'expand', u'exampl', u'less', u'paul', u'web', u'five', u'corpor', u'crisi', u'dynegi', u'feder', u'avoid', u'stage', u'sander', u'otherwis', u'problem', u'novemb', u'inc', u'monday', u'rule', u'worth', u'told', u'studi', u'total', u'sarah', u'word', u'work', u'could', u'indic', u'interview', u'lay', u'law', u'order', u'offici', u'offic', u'bank', u'standard', u'renew', u'independ', u'john', u'provid', u'pressur', u'minut', u'provis', u'opposit', u'identifi', u'involv', u'secretari', u'septemb', u'awar', u'away', u'accord', u'howev', u'speak', u'revenu', u'properti', u'air', u'evid', u'hand', u'rais', u'thu', u'client', u'jose', u'board', u'night', u'plu', u'confer', u'post', u'mahoney', u'opportun', u'way', u'jone', u'consum', u'certainli', u'brown', u'octob', u'administr', u'guy', u'cost', u'appear', u'gener', u'market', u'live', u'prepar', u'cap', u'focu', u'cal', u'heard', u'occur', u'economi', u'write', u'product', u'aol', u'southern', u'approv', u'still', u'non', u'half', u'hall', u'drop', u'januari', u'bradley', u'year', u'happen', u'argu', u'care', u'directli', u'tomorrow', u'especi', u'angel', u'deliv', u'begin', u'price', u'america', u'cannot', u'concern', u'import', u'blame', u'mccubbin', u'mona', u'senat', u'media', u'ceo', u'document', u'someon', u'earli', u'read', u'maureen', u'rebecca', u'benefit', u'central', u'area', u'lon', u'start', u'low', u'lot', u'hire', u'stanford', u'peak', u'reduc', u'month', u'articl', u'smith', u'blackout', u'pacif', u'excess', u'strong', u'ahead', u'amount', u'real', u'purchas', u'hartso', u'almost', u'taken', u'anoth', u'help', u'soon', u'held', u'fulli', u'democrat', u'beyond', u'event', u'robert', u'publish', u'puc', u'reason', u'base', u'put', u'basi', u'american', u'lawmak', u'toward', u'clear', u'latest', u'copyright', u'close', u'particip', u'won', u'jeff', u'coordin', u'look', u'invit', u'governor', u'readi', u'grant', u'user', u'march', u'game', u'resolv', u'donna', u'run', u'step', u'block', u'within', u'incent', u'institut', u'similar', u'doesn', u'repres', u'guarante', u'clean', u'william', u'megawatt', u'michael', u'click', u'becom', u'convers', u'chang', u'chanc', u'win', u'control', u'georg', u'agenda', u'handl', u'special', u'manipul', u'ect', u'ask', u'keep', u'attach', u'final', u'deliveri', u'need', u'happi', u'urg', u'affair', u'fuel', u'buyer', u'gray', u'contain', u'statu', u'ken', u'key', u'monitor', u'polici', u'tuesday', u'quit', u'cent', u'revis', u'parti', u'speaker', u'northwest', u'http', u'effect', u'well', u'skill', u'loss', u'lost', u'necessari', u'page', u'home', u'peter', u'estim', u'encourag', u'washington', u'north', u'limit', u'eric', u'diego', u'futur', u'stay', u'decemb', u'whose', u'guerrero', u'govern', u'affect', u'even', u'neg', u'spokesman', u'new', u'net', u'ever', u'never', u'met', u'cindi', u'jame', u'permit', u'counti', u'campaign', u'call', u'recommend', u'robertson', u'type', u'tell', u'schroeder', u'warn', u'room', u'give', u'answer', u'negoti', u'updat', u'attempt', u'third', u'maintain', u'privileg', u'better', u'promis', u'went', u'side', u'bond', u'situat', u'dasovich', u'iso', u'isn', u'agre', u'lower', u'matter', u'seen', u'seem', u'seek', u'kean', u'doc', u'despit', u'explain', u'stop', u'bad', u'subject', u'said', u'three', u'commission', u'interest', u'basic', u'servic', u'near', u'receiv', u'make', u'skean', u'sacramento', u'left', u'yet', u'languag', u'nicolay', u'save', u'applic', u'www', u'deal', u'intern', u'dear', u'promot', u'afternoon', u'commit', u'analysi', u'form', u'forc', u'analyst', u'sale', u'billion', u'ignor', u'daili', u'mile', u'depend', u'lynch', u'item', u'wait', u'extrem', u'bob', u'elect', u'visit', u'effort', u'claim', u'kenneth', u'predict', u'council', u'agenc', u'may', u'grow', u'talk', u'entiti', u'group', u'thank', u'mail', u'main', u'financi', u'initi', u'continu', u'ensur', u'earlier', u'california', u'org', u'thing', u'think', u'first', u'carri', u'long', u'oppos', u'averag', u'kevin', u'advis', u'track', u'paid', u'show', u'black', u'nearli', u'variou', u'get', u'calendar', u'wonder', u'enough', u'volatil', u'across', u'august', u'trader', u'improv', u'among', u'mara', u'mari', u'mark', u'univers', u'sound', u'margin', u'advantag', u'pat', u'pay', u'jeffrey', u'oil', u'assist', u'appropri', u'execut', u'money', u'grid', u'either', u'confirm', u'possibl', u'file', u'network', u'dollar', u'util', u'fall', u'abl', u'public', u'valu', u'declin', u'establish', u'two', u'desir', u'hous', u'remain', u'secur', u'associ', u'late', u'pend', u'good', u'hard', u'idea', u'connect', u'energi', u'exchang', u'done', u'construct', u'statement', u'part', u'build', u'distribut', u'previou', u'charl', u'charg', u'commerc', u'particularli', u'fine', u'find', u'steff', u'express', u'vinc', u'reserv', u'expert', u'someth', u'experi', u'altern', u'appreci', u'annual', u'simpli', u'point', u'suppli', u'throughout', u'understand', u'bill', u'everyon', u'yesterday', u'spent', u'spend', u'cut', u'big', u'bid', u'often', u'back', u'per', u'larg', u'impos', u'bankruptci', u'steven', u'includ', u'forward', u'privat', u'refer', u'edison', u'access', u'peggi', u'meet', u'sinc', u'later', u'advoc', u'account', u'employe', u'onlin', u'stock', u'chief', u'dan', u'mention', u'day', u'februari', u'rel', u'frank', u'releas', u'presid', u'retail', u'south', u'attent', u'facil', u'messag', u'structur', u'mid', u'unless', u'payment', u'request', u'texa', u'staff', u'increas', u'organ', u'shapiro', u'progress', u'janet', u'janel', u'comment', u'electr', u'partner', u'center', u'thought', u'add', u'fyi', u'like', u'success', u'hain', u'although', u'panel', u'actual', u'biggest', u'buy', u'probabl', u'detail', u'everi', u'risk', u'rise', u'school', u'direct', u'street', u'conduct', u'supplier', u'forum', u'describ', u'would', u'asset', u'spike', u'phone', u'must', u'join', u'end', u'elizabeth', u'enter', u'choic', u'goe', u'got', u'gov', u'free', u'rang', u'alreadi', u'top', u'tom', u'took', u'western', u'conserv', u'ray', u'contact', u'though', u'flow', u'report', u'approach', u'news', u'protect', u'karen', u'quickli', u'tabl', u'suggest', u'complet', u'everyth', u'els', u'gave', u'cooper', u'corp', u'produc', u'duke', u'ferc', u'head', u'heat', u'hear', u'counsel', u'kaufman', u'check', u'assembl', u'tim', u'consid', u'longer', u'rob', u'signific', u'environment', u'sourc', u'hour', u'level', u'quick', u'repli', u'water', u'wholesal', u'today', u'capit', u'share', u'judg', u'purpos', u'critic', u'alway', u'anyon', u'huge', u'practic', u'inform', u'combin', u'refund', u'term', u'name', u'individu', u'profit', u'refus', u'place', u'origin', u'given', u'white', u'copi', u'alan', u'shirley', u'serv', u'wide', u'balanc', u'posit', u'ann', u'sure', u'senior', u'brigg', u'state', u'wood', u'review', u'comn', u'come', u'region', u'contract', u'period', u'coupl', u'andrew', u'case', u'amend', u'cash', u'author', u'week', u'confidenti', u'director', u'without', u'relief', u'except', u'legal', u'sherriff', u'stand', u'gari', u'regard', u'mike', u'belden', u'invest', u'potenti', u'lack', u'abil', u'follow', u'thursday', u'program', u'fax', u'global', u'far', u'list', u'rate', u'design', u'sue', u'brief', u'version', u'proceed', u'known', u'enronxg', u'cours', u'court', u'goal', u'rather', u'resourc', u'reflect', u'short', u'susan', u'caus', u'began', u'style', u'might', u'wouldn', u'return', u'expect', u'hill', u'advanc', u'friday', u'differ', u'perhap', u'feel', u'least', u'stori', u'option', u'kind', u'chairman', u'reach', u'sandra', u'hit', u'financ', u'due', u'strategi', u'demand', u'higher', u'propos', u'reliant', u'collect', u'pipelin', u'surpris', u'seller', u'palmer', u'novosel', u'announc', u'question', u'perform', u'consist', u'vice', u'steve', u'edu', u'pleas', u'compar', u'wolak', u'appli', u'greg', u'use', u'sort', u'tax', u'sit', u'six', u'instead', u'chri', u'attend', u'light', u'republican', u'investor', u'polit', u'york', u'capac', u'shelk', u'juli', u'deni', u'denn', u'determin', u'locat', u'local', u'contribut', u'view', u'favor', u'focus', u'job', u'entir', u'joe', u'addit', u'april', u'committe', u'wall', u'respect', u'present', u'will', u'member', u'largest', u'difficult', u'obtain', u'discuss', u'press', u'growth', u'lead', u'leav', u'leader', u'investig', u'own', u'owe', u'made', u'whether', u'troubl', u'record', u'percent', u'book', u'june', u'reliabl', u'emerg', 'LABELS']\n"
     ]
    }
   ],
   "source": [
    "print templist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "templist[0].append('LABELS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enronLabels = [x[0] for x in enronEmailsClean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1702\n",
      "1703\n"
     ]
    }
   ],
   "source": [
    "print len(enronLabels)\n",
    "print len(templist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, value in enumerate(templist[1:]):\n",
    "    value.append(enronLabels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wholeEnronData = templist[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1703\n",
      "1060\n",
      "1060\n",
      "1060\n"
     ]
    }
   ],
   "source": [
    "print len(wholeEnronData)\n",
    "print len(wholeEnronData[0])\n",
    "print len(wholeEnronData[1])\n",
    "print len(wholeEnronData[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enronDataAnalysis = pd.DataFrame(wholeEnronData[1:], columns=wholeEnronData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>four</th>\n",
       "      <th>wednesday</th>\n",
       "      <th>budget</th>\n",
       "      <th>second</th>\n",
       "      <th>deregul</th>\n",
       "      <th>brought</th>\n",
       "      <th>unit</th>\n",
       "      <th>spoke</th>\n",
       "      <th>relat</th>\n",
       "      <th>notic</th>\n",
       "      <th>...</th>\n",
       "      <th>made</th>\n",
       "      <th>whether</th>\n",
       "      <th>troubl</th>\n",
       "      <th>record</th>\n",
       "      <th>percent</th>\n",
       "      <th>book</th>\n",
       "      <th>june</th>\n",
       "      <th>reliabl</th>\n",
       "      <th>emerg</th>\n",
       "      <th>LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1060 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   four  wednesday  budget  second  deregul  brought  unit  spoke  relat  \\\n",
       "0     0          0       0       1        0        0     0      0      0   \n",
       "1     0          0       1       0        0        0     0      0      0   \n",
       "2     0          2       0       0        1        0     0      0      0   \n",
       "3     0          0       0       0        0        0     0      0      2   \n",
       "4     0          0       0       0        0        0     0      0      0   \n",
       "\n",
       "   notic   ...    made  whether  troubl  record  percent  book  june  reliabl  \\\n",
       "0      0   ...       0        0       0       0        0     0     0        0   \n",
       "1      0   ...       0        2       0       0        0     0     0        0   \n",
       "2      0   ...       0        1       0       1        1     0     0        0   \n",
       "3      1   ...       0        0       0       0        0     0     4        0   \n",
       "4      0   ...       0        0       0       0        0     0     0        0   \n",
       "\n",
       "   emerg  LABELS  \n",
       "0      0       1  \n",
       "1      0       0  \n",
       "2      1       1  \n",
       "3      0       0  \n",
       "4      0       0  \n",
       "\n",
       "[5 rows x 1060 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enronDataAnalysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enronDataAnalysis.to_csv('usableenron.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
